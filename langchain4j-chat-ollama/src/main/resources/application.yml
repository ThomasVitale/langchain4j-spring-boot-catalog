langchain4j:
  ollama:
    client:
      log-requests: true
      log-responses: true
    chat:
      model: mistral
      options:
        temperature: 0.7
