langchain4j:
  ollama:
    client:
      log-requests: true
      log-responses: true
    chat:
      model: llama3
      options:
        temperature: 0.7
    embedding:
      model: llama3
